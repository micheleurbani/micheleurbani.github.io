<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A multi-policy hyper heuristic for multiobjective optimization | Michele Urbani </title> <meta name="author" content="Michele Urbani"> <meta name="description" content="A poster for the GECCO23 Conference, July 15-19th, 2023, Lisbon"> <meta name="keywords" content="operations research, optimization, computational intelligence, artificial intelligence"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mikiurbi.github.io//projects/gecco23-poster/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "A multi-policy hyper heuristic for multiobjective optimization",
            "description": "A poster for the GECCO23 Conference, July 15-19th, 2023, Lisbon",
            "published": "February 23, 2024",
            "authors": [
              
              {
                "author": "Michele Urbani",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Department of Industrial Engineering, University of Trento",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Francesco Pilati",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Department of Industrial Engineering, University of Trento",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Michele</span> Urbani </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>A multi-policy hyper heuristic for multiobjective optimization</h1> <p>A poster for the GECCO23 Conference, July 15-19th, 2023, Lisbon</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#motivations">Motivations</a> </div> <div> <a href="#research-goals">Research goals</a> </div> <div> <a href="#methodology">Methodology</a> </div> <div> <a href="#algorithm-architecture">Algorithm architecture</a> </div> <div> <a href="#experimental-setting">Experimental setting</a> </div> <div> <a href="#selected-results">Selected results</a> </div> <div> <a href="#conclusions-and-future-research">Conclusions and future research</a> </div> </nav> </d-contents> <p>Selection hyper-heuristics (SSHH) are search strategies that can be successfully applied to multi-objective optimization (MOO) problems. They showed resilient results to different types of optimization problems, which may come at the cost of a longer resolution time than other metaheuristics. Learning a single selection rule in MOO might be limiting because the information from the multiple objectives might be unexploited. A novel approach named the multi-policy approach is proposed and discussed to further enhance the searching ability of sequence-based selection hyper-heuristics, together with an ad-hoc learning rule. The availability of a set of problem-specific low-level heuristics is assumed and the resolution of a specific class of combinatorial problems, i.e., vehicle routing problems (VRPs), is considered for a numerical example. The multi-policy approach showed the ability to learn different selection rules for each objective and to change them during the execution of the algorithm, i.e., when solutions are getting closer to the Pareto front. The availability of parallel computation can speed up the calculation since the methodology is suitable for parallelization. The proposed methodology was found to produce production-ready solutions and the approach is expected to be successfully extended to problems different from the VRP.</p> <h2 id="motivations">Motivations</h2> <ul> <li>Selection hyper-heuristics (SSHH) are search strategies that can be successfully applied to multiobjective optimization problems<d-cite key="DRAKE2020405"></d-cite>.</li> <li>Learning a single selection rule in a SSHH might be limiting because the information from the multiple objectives might be unexploited<d-cite key="TRICOIRE20123089"></d-cite>.</li> <li>Algorithms based on sequences of problem-specific low-level heuristics can efficiently solve complex combinatorial problem<d-cite key="kheiri2020inventory"></d-cite>.</li> </ul> <h2 id="research-goals">Research goals</h2> <ul> <li>A novel approach named the <strong>multi-policy</strong> approach is proposed to further enhance the searching ability of sequence-based selection hyper-heuristics.</li> <li>The multi-policy approach performs <strong>online</strong> learning of the select policies. One selection policy per objective is learned using objective-wise information.</li> <li>The proposed algorithm is tested on a <strong>real-world</strong> instance of the vehicle routing problem with pickup and delivery (VRPPD).</li> </ul> <h2 id="methodology">Methodology</h2> <p>A <strong>low-level heuristic</strong> (LLH) is a rule that modifies the decision variables \(\mathbf{z}\) of the problem under analysis. A set \(H\) of \(m\) LLHs is assumed to be available to modify the solutions \(\mathbf{z} \in Pop\).</p> <p>A <strong>selection policy</strong> is an ensemble of Markov decision models that alternates the selection of LLHs \(h\) and sequence-termination signals \(AS\) (see box 1 in Figure 1). There are <strong>as many selection policies as the number $N$ of objectives</strong>.</p> <p>At each iteration, a selection policy produces a sequence of low-level heuristics \(SEQ\) (see box 2 in Figure 1) to be applied to a solution \(\mathbf{z}\).</p> <p>For each new solution \(\mathbf{z'} \in Pop'\) that was improved by a sequence of heuristics \(SEQ\), the <strong>reward rule</strong> (see box 3 in Figure 1) attributes a score to all the couples of subsequent heuristics in \(SEQ\).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/selection-policy-1-480.webp 480w,/assets/img/selection-policy-1-800.webp 800w,/assets/img/selection-policy-1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/selection-policy-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: The architecture of the learning system. </div> <h2 id="algorithm-architecture">Algorithm architecture</h2> <p>The multi-policy hyper heuristic foresees an initiation phase followed by the main loop. The main loop is executed separately for each policy, which makes the procedure suitable to be parallelized. The termination criterion chosen for the poster was the number of iterations; time and convergence of a performance metric can be used as well.</p> <p>As shown in Figure 2, the main loop is followed by the update of selection policies and a check for the termination condition.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mphh-architecture-1-480.webp 480w,/assets/img/mphh-architecture-1-800.webp 800w,/assets/img/mphh-architecture-1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/mphh-architecture-1.png" class="img-fluid rounded z-depth-1" width="400" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: The architecture of the multi-policy hyper heuristic. </div> <h2 id="experimental-setting">Experimental setting</h2> <p>Numerical experiments were carried out to solve a three-objective version of the vehicle routing problem with pickup and delivery (VRPPD).</p> <p>The multi-policy algorithm was tested on a single instance of the VRPPD with <strong>60 deliveries</strong> and <strong>4 pickup points</strong>. Figure 3 shows an example of the Pareto front obtained by the multi-policy algorithm.</p> <div class="l-page"> <iframe src="/assets/plotly/pareto.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Figure 3: A Pareto front obtained by the multi-policy algorithm. </div> <p>Data are inspired to a <strong>real-world case study</strong>: a geography with non-Euclidean distances was used, and goods to be delivered presented different weights and volumes.</p> <h2 id="selected-results">Selected results</h2> <ol> <li>Selection policies evolve over time: for example, comparing \(\mathbf{p}^{(eco)}\) at iteration 25 and 50 in Figure 4, the selection probability distribution \(p_{h_4}\) changes significantly.</li> <li>Selection policies evolve differently from each other: for instance, \(\mathbf{p}^{(eco)}\) and \(\mathbf{p}^{(env)}\) at iteration 50 in Figure 4 and 5 present significantly different values for \(p_{h_0}\) and \(p_{h_1}\).</li> <li>The learned policies are sensitive to: (1) the <strong>reward value</strong> \(r\) that is used, (2) the number of iterations between <strong>policy updates</strong> \(N_{update}\), and (3) the weight that is used when updating the probabilities in the Markov decision model.</li> </ol> <div class="l-page"> <iframe src="/assets/plotly/economic.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Figure 4: A representation of the low-level heuristic selection policy for the economic objecetive. </div> <div class="l-page"> <iframe src="/assets/plotly/environmental.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Figure 5: A representation of the low-level heuristic selection policy for the environmental objecetive. </div> <div class="l-page"> <iframe src="/assets/plotly/social.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Figure 6: A representation of the low-level heuristic selection policy for the social objecetive. </div> <h2 id="conclusions-and-future-research">Conclusions and future research</h2> <ul> <li> <strong>Learning selection policies</strong> is not straightforward. Extensive testing of the learning rule is required over a large and diverse set of instances of the problem studied.</li> <li> <strong>Online learning</strong> in multiobjective combinatorial problems is <strong>time expensive</strong>. Either a high-performance implementation of the algorithm exists, or offline learning should be considered.</li> <li> <strong>Comparative analysis</strong> of the proposed multi-policy hyper heuristic is required. Algorithms such as multiobjective local search (MOLS) and choice function hyper heuristic (CFHH) will be considered for testing over <strong>large</strong> (e.g., up to 500 nodes) and <strong>diverse</strong> (e.g. geography, load, time windows) <strong>problem instances</strong>.</li> <li> <strong>Sampling of LLH sequences</strong> yield widely variable execution times when the termination criterion is the number of iterations, which might be undesirable in the production phase.</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/gecco23.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2024 Michele Urbani. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>